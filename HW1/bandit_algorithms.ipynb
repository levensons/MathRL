{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #1: Bandit algorithms"
      ],
      "metadata": {
        "id": "WyJ94DaAQ0hY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPk01HXLQzxr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/rlberry-py/rlberry.git@v0.3.0#egg=rlberry[default] "
      ],
      "metadata": {
        "id": "-z5n9vnGQ6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rlberry.agents.bandits import BanditWithSimplePolicy\n",
        "from rlberry.wrappers import WriterWrapper"
      ],
      "metadata": {
        "id": "hv88YGHuQ7AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem #1: Gaussian bandits"
      ],
      "metadata": {
        "id": "ZyMSZfH3Q_c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the bandit problem where each arm follows normal distribution with its own mean and variance $\\mathcal{D}_a \\sim \\mathcal{N}(\\mu_a, \\sigma^2_a)$, where both parameters $\\mu_a$ and $\\sigma^2_a$ may differs for all arms."
      ],
      "metadata": {
        "id": "wvuw7wAtREjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment, documentation: https://rlberry.readthedocs.io/en/latest/generated/rlberry.envs.bandits.NormalBandit.html\n",
        "from rlberry.envs.bandits import NormalBandit"
      ],
      "metadata": {
        "id": "aYimu0ePReWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, the task follows:\n",
        "- Implement UCB-1, Thompson sampling and KL-UCB algorithms for this problem;\n",
        "- Study different properties of these algorithms."
      ],
      "metadata": {
        "id": "8RqAqhW6SO9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UCB-1\n",
        "All the theory for this algorithm was given, you suppose to prove that it also has nearly optimal regret even for sub-gaussian distribution (thus for gaussian too). Feel free to change method signature as you want. \n",
        "\n",
        "**Task**: implement it."
      ],
      "metadata": {
        "id": "nlJyu-YWS6Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UCBAgent(BanditWithSimplePolicy):\n",
        "    \"\"\"\n",
        "        UCB-1 agent for gaussian bandits.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sigma: sub-gaussian parameter\n",
        "        env: rlberry bandit environment\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"UCB-1\"\n",
        "\n",
        "    def __init__(self, env, sigma, **kwargs):\n",
        "        \"\"\"\n",
        "            Usefull fields of base class:\n",
        "            * self.n_arms : number of arms\n",
        "        \"\"\"\n",
        "        BanditWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        # To track statistics on chosen actions\n",
        "        self.env = WriterWrapper(self.env, self.writer, write_scalar=\"action\")\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, budget=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit function for greedy agent\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Total number of iterations, also called horizon.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "KJsGTiTZTKfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thompson sampling\n",
        "\n",
        "Now we consider the algorithm that is not based on Optimism in the face of uncertainty principle but can be very efficient on many environments.\n",
        "\n",
        "Let us fix one arm $a$. Then rewards $r$ follows some parametric distribution $p(r | \\theta_a)$, where the family of distribution is the same for all arms. Let us consider some \\textit{prior} distribution over parameters $\\rho(\\theta_a)$. Given sampled rewards $r_1,\\ldots, r_n$, we may define a posterior distribution of parameter $\\theta_a$ as follows\n",
        "$$\n",
        "    p(\\theta | r_1,\\ldots,r_n) = \\frac{\\prod_{i=1}^n p(r_i | \\theta) \\rho(\\theta)}{\\int_{} \\prod_{i=1}^n p(r_i | \\theta) \\rho(\\theta) d \\theta}\n",
        "$$\n",
        "Then Thompson Sampling (TS) algorithm propose the following:\n",
        "- Sample $\\theta^t_a$ from the postrior distribution for all arms $a \\in \\mathcal{A}$;\n",
        "- Select $\\hat a^t = \\arg\\max_{a} \\mathbb{E}[r | \\theta^t_a ] = \\arg\\max_{a} \\int r p(r | \\theta^t_a) d r$, in other words, we find the optimal arm while treating sampled parameters as real one.\n",
        "- Update the posterior distribution for arm $\\hat a^t$ given new reward $r_t$.\n",
        "\n",
        "\n",
        "\n",
        "Usually, computation of posterior distribution is intractable, however, if we have conjugate prior, we may efficiently compute it. For this particular problem reward follows normal distribution with parameters $\\mu_a, \\sigma^2_a$. It is well-known that  there is a conjugate prior for normal distribution, so, TS for gaussian bandits is tractable.\n",
        "\n",
        "**Task**: implement TS algorithm for two cases:\n",
        "- Assume that variance for all distributions of arms is given. In this case, you may assume Bayessian model only for mean $\\mu_a$ and define prior as normal distribution.\n",
        "- Implement variance-adaptive version of TS algorithm. In this case you have two parameters for Bayesian model, for which you have normal-inverse-gamma distribution https://en.wikipedia.org/wiki/Normal-inverse-gamma_distribution \n",
        "\n",
        "Parameters of prior distribution should be treated as hyperparameters and study dependence of them on performance of obtained algorithms."
      ],
      "metadata": {
        "id": "6vBI2DSTTdCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TSNormalAgent(BanditWithSimplePolicy):\n",
        "    \"\"\"\n",
        "        TS agent for Gaussian bandits with known variance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        params: parameters of prior \n",
        "        env: rlberry bandit environment\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"TS\"\n",
        "\n",
        "    def __init__(self, env, params, **kwargs):\n",
        "        \"\"\"\n",
        "            Usefull fields of base class:\n",
        "            * self.n_arms : number of arms\n",
        "        \"\"\"\n",
        "        BanditWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        # To track statistics on chosen actions\n",
        "        self.env = WriterWrapper(self.env, self.writer, write_scalar=\"action\")\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, budget=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit function for greedy agent\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Total number of iterations, also called horizon.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "SbvcdeDhZfnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSNormalAgentVarianceAdaptive(BanditWithSimplePolicy):\n",
        "    \"\"\"\n",
        "        TS agent for Gaussian bandits with unknown variance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        params: parameters of prior \n",
        "        env: rlberry bandit environment\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"AdaptiveTS\"\n",
        "\n",
        "    def __init__(self, env, prarams, **kwargs):\n",
        "        \"\"\"\n",
        "            Usefull fields of base class:\n",
        "            * self.n_arms : number of arms\n",
        "        \"\"\"\n",
        "        BanditWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        # To track statistics on chosen actions\n",
        "        self.env = WriterWrapper(self.env, self.writer, write_scalar=\"action\")\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, budget=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit function for greedy agent\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Total number of iterations, also called horizon.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "9LLw_5ufzAZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KL-UCB\n",
        "\n",
        "Finally, you are going to implement the last baseline that is called KL-UCB. This algorirthm is based on Optimism in the face of uncertainty principle, but in much more involved form. Basically, algorithm is following:\n",
        "\n",
        "1) Warm-up: take each arm one time to obtain mean estimates;\n",
        "\n",
        "2) Algorithm:\n",
        "- Compute $\\hat \\mu^t(a)$ for all arms (just empirical estimate of the mean);\n",
        "- Compute \n",
        "$$\n",
        "    \\tilde \\mu^t(a) = \\max_{q \\in [0,1]} \\{q : n^t(a) \\cdot \\mathrm{kl}(\\hat \\mu^t(a) || q) \\leq \\log(t) + c \\log \\log (t)\\},\n",
        "$$ where $\\mathrm{kl}(a||b)$ is binary KL-divergence defined as $\\mathrm{kl}(a||b) = a \\log(a/b) + (1-a) \\log((1-a)/(1-b))$.\n",
        "- Play $\\hat a^t = \\arg\\max_{a} \\tilde \\mu^t(a)$;\n",
        "- Update all statistics.\n",
        "\n",
        "\n",
        "The step of computation of $q$ requires binary search. In practice usually $c$ equals to $0$ whereas theory prescribed choose it as a very large constant. Unfortunately, this particular version of the algorithm works only for bounded distribution, which is not the case of Gaussian distribution.\n",
        "\n",
        "\n",
        "However, as it described in Section 4 of the original KL-UCB paper https://arxiv.org/abs/1102.2490, this algorithm may be applied not only to bounded distributions, but also to so-called *single-parameter exponential family* algorithm, for which the (generalized) density has the following form\n",
        "$$\n",
        "    p(x | \\theta) = \\exp(x \\cdot \\theta - b(\\theta) + c(x)).\n",
        "$$\n",
        "For example, normal distribution with fixed variance satisfies satisfies it. In this case we may replace binary KL divergence by\n",
        "$$\n",
        "    d(x, \\mu(\\theta)) = \\sup_{\\lambda} \\left( \\lambda x - \\log \\mathbb{E}_\\theta[\\lambda X] \\right),\n",
        "$$\n",
        "where $\\mu(\\theta)$ is an expectation of $p(x|\\theta)$ given parameter theta. In the case of single-parameter exponential familty you have one-to-one corresponce between $\\mu(\\theta)$ and $\\theta$, so this function is well-defined.\n",
        "\n",
        "**Task**.\n",
        "- Implement KL-UCB for bounded distributions, check it convergence on Bernulli bandit case (seminar notebook). \n",
        "- Learn how to run bounded version of KL-UCB on any fixed interval (not only [0,1]) and apply it to Gaussian bandits by fixing some interval of your choice and applying clipping of reward;\n",
        "- Compute $d(x, \\mu(\\theta))$ for Gaussian distributions in the closed form.\n",
        "- Implement KL-UCB for Gaussian distributions (with fixed variance);"
      ],
      "metadata": {
        "id": "2VEVIGJNZfKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KLUCBBoundedAgent(BanditWithSimplePolicy):\n",
        "    \"\"\"\n",
        "        KL-UCB agent for bounded bandits.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        segment: tuple (a,b) with the interval of truncation. By default (0,1)\n",
        "        env: rlberry bandit environment\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"TS\"\n",
        "\n",
        "    def __init__(self, env, segment=(0,1), **kwargs):\n",
        "        \"\"\"\n",
        "            Usefull fields of base class:\n",
        "            * self.n_arms : number of arms\n",
        "        \"\"\"\n",
        "        BanditWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        # To track statistics on chosen actions\n",
        "        self.env = WriterWrapper(self.env, self.writer, write_scalar=\"action\")\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, budget=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit function for greedy agent\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Total number of iterations, also called horizon.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "w1JMzXWLYGXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KLUCBGaussianAgent(BanditWithSimplePolicy):\n",
        "    \"\"\"\n",
        "        KL-UCB agent for Gaussian bandits with known variance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sigma_sq: variance of arms\n",
        "        env: rlberry bandit environment\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"TS\"\n",
        "\n",
        "    def __init__(self, env, sigma_sq, **kwargs):\n",
        "        \"\"\"\n",
        "            Usefull fields of base class:\n",
        "            * self.n_arms : number of arms\n",
        "        \"\"\"\n",
        "        BanditWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        # To track statistics on chosen actions\n",
        "        self.env = WriterWrapper(self.env, self.writer, write_scalar=\"action\")\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, budget=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit function for greedy agent\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Total number of iterations, also called horizon.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "7nE4vekrYGs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Comparison\n",
        "\n",
        "You have to do the following comparisons:\n",
        "\n",
        "- Compare Thompson sampling with and without variance adaptation on Gaussian bandits with all different variance for all arms it two cases: there is no misspecification in variance (first TS algorithm have exact information) and there is misspecification (first TS algorithm obtains some upper bound on all variance). Study how much misspecification affects performance.\n",
        "- Compare KL-UCB for bounded distributions with truncation and Gaussian one. Study dependence on the range of truncation on the performance of the algorithm.\n",
        "- Finally, compare UCB-1, Thompson Sampling, and KL-UCB in the setting of known variance. Make conclusion."
      ],
      "metadata": {
        "id": "UHQdyG6uYG9l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vvGU10J4au42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}