{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fZctv0HAowr",
        "outputId": "84090c93-ab06-425e-9a05-e89b2ead698b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mujoco\n",
            "  Downloading mujoco-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.6.0)\n",
            "Collecting glfw (from mujoco)\n",
            "  Downloading glfw-2.6.4-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.23.5)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n",
            "Installing collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.6.4 mujoco-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_3pbWZfdF__",
        "outputId": "69bf7551-67bc-418a-9444-0ba251f08802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting environment variable to use GPU rendering:\n",
            "env: MUJOCO_GL=egl\n",
            "Checking that the installation succeeded:\n",
            "Installation successful.\n"
          ]
        }
      ],
      "source": [
        "#@title Set up rendering, check installation\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "PRwMNuvir5g5",
        "outputId": "48d49668-fc1d-4a42-aa61-6e95e2fa81b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ale-py~=0.7.5 (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2)\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2)\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting mujoco==2.2.0 (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2)\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2.31.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (1.4.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2.6.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (3.1.7)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (6.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic_control,mujoco]==0.25.2) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=7e2ae0b7da55ed6bec6fbaa82cb2742af86c21435065f200a7d879f89f4e8699\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: pygame, mujoco, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: mujoco\n",
            "    Found existing installation: mujoco 3.1.0\n",
            "    Uninstalling mujoco-3.1.0:\n",
            "      Successfully uninstalled mujoco-3.1.0\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.7.5 autorom-0.4.2 mujoco-2.2.0 pygame-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mujoco"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gym[classic_control,mujoco,atari,accept-rom-license]==0.25.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYS5mLAqt5Xa"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, Callable, Tuple, Optional, Union, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "import cv2\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym.wrappers.record_episode_statistics import RecordEpisodeStatistics\n",
        "from gym.wrappers.rescale_action import RescaleAction\n",
        "from gym.wrappers.clip_action import ClipAction\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs7Ymzowbft-",
        "outputId": "5af58e49-f280-4a42-9611-f0fc3b0bb835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# from mujoco_py import GlfwContext\n",
        "# GlfwContext(offscreen=True)  # Create a window to init GLFW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXemii7l855V"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6cyDlR598Kt"
      },
      "outputs": [],
      "source": [
        "def from_numpy(data: Union[np.ndarray, dict], **kwargs):\n",
        "    if isinstance(data, dict):\n",
        "        return {k: from_numpy(v) for k, v in data.items()}\n",
        "    else:\n",
        "        data = torch.from_numpy(data, **kwargs)\n",
        "        if data.dtype == torch.float64:\n",
        "            data = data.float()\n",
        "        return data.to(device)\n",
        "\n",
        "def to_numpy(tensor: Union[torch.Tensor, dict]):\n",
        "    if isinstance(tensor, dict):\n",
        "        return {k: to_numpy(v) for k, v in tensor.items()}\n",
        "    else:\n",
        "        return tensor.to(\"cpu\").detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn9o_OtbsWFg"
      },
      "outputs": [],
      "source": [
        "class SoftActorCritic(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape: Sequence[int],\n",
        "        action_dim: int,\n",
        "        make_actor: Callable[[Tuple[int, ...], int], nn.Module],\n",
        "        make_actor_optimizer: Callable[[torch.nn.ParameterList], torch.optim.Optimizer],\n",
        "        make_actor_schedule: Callable[\n",
        "            [torch.optim.Optimizer], torch.optim.lr_scheduler._LRScheduler\n",
        "        ],\n",
        "        make_critic: Callable[[Tuple[int, ...], int], nn.Module],\n",
        "        make_critic_optimizer: Callable[\n",
        "            [torch.nn.ParameterList], torch.optim.Optimizer\n",
        "        ],\n",
        "        make_critic_schedule: Callable[\n",
        "            [torch.optim.Optimizer], torch.optim.lr_scheduler._LRScheduler\n",
        "        ],\n",
        "        discount: float,\n",
        "        target_update_period: Optional[int] = None,\n",
        "        soft_target_update_rate: Optional[float] = None,\n",
        "        # Actor-critic configuration\n",
        "        actor_gradient_type: str = \"reinforce\",  # One of \"reinforce\" or \"reparametrize\"\n",
        "        num_actor_samples: int = 1,\n",
        "        num_critic_updates: int = 1,\n",
        "        # Settings for multiple critics\n",
        "        num_critic_networks: int = 1,\n",
        "        target_critic_backup_type: str = \"mean\",  # One of \"doubleq\", \"min\", \"redq\", or \"mean\"\n",
        "        # Soft actor-critic\n",
        "        use_entropy_bonus: bool = False,\n",
        "        temperature: float = 0.0,\n",
        "        backup_entropy: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert target_critic_backup_type in [\n",
        "            \"doubleq\",\n",
        "            \"min\",\n",
        "            \"mean\",\n",
        "            \"redq\",\n",
        "        ], f\"{target_critic_backup_type} is not a valid target critic backup type\"\n",
        "\n",
        "        assert actor_gradient_type in [\n",
        "            \"reinforce\",\n",
        "            \"reparametrize\",\n",
        "        ], f\"{actor_gradient_type} is not a valid type of actor gradient update\"\n",
        "\n",
        "        assert (\n",
        "            target_update_period is not None or soft_target_update_rate is not None\n",
        "        ), \"Must specify either target_update_period or soft_target_update_rate\"\n",
        "\n",
        "        self.actor = make_actor(observation_shape, action_dim)\n",
        "        self.actor_optimizer = make_actor_optimizer(self.actor.parameters())\n",
        "        self.actor_lr_scheduler = make_actor_schedule(self.actor_optimizer)\n",
        "\n",
        "        self.critics = nn.ModuleList(\n",
        "            [\n",
        "                make_critic(observation_shape, action_dim)\n",
        "                for _ in range(num_critic_networks)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.critic_optimizer = make_critic_optimizer(self.critics.parameters())\n",
        "        self.critic_lr_scheduler = make_critic_schedule(self.critic_optimizer)\n",
        "        self.target_critics = nn.ModuleList(\n",
        "            [\n",
        "                make_critic(observation_shape, action_dim)\n",
        "                for _ in range(num_critic_networks)\n",
        "            ]\n",
        "        )\n",
        "        self.update_target_critic()\n",
        "\n",
        "        self.observation_shape = observation_shape\n",
        "        self.action_dim = action_dim\n",
        "        self.discount = discount\n",
        "        self.target_update_period = target_update_period\n",
        "        self.target_critic_backup_type = target_critic_backup_type\n",
        "        self.num_critic_networks = num_critic_networks\n",
        "        self.use_entropy_bonus = use_entropy_bonus\n",
        "        self.temperature = temperature\n",
        "        self.actor_gradient_type = actor_gradient_type\n",
        "        self.num_actor_samples = num_actor_samples\n",
        "        self.num_critic_updates = num_critic_updates\n",
        "        self.soft_target_update_rate = soft_target_update_rate\n",
        "        self.backup_entropy = backup_entropy\n",
        "\n",
        "        self.critic_loss = nn.MSELoss()\n",
        "\n",
        "    def get_action(self, observation: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the action for a given observation.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            observation = from_numpy(observation)[None]\n",
        "\n",
        "            action_distribution: torch.distributions.Distribution = self.actor(observation)\n",
        "            action: torch.Tensor = action_distribution.sample()\n",
        "\n",
        "            assert action.shape == (1, self.action_dim), action.shape\n",
        "            return to_numpy(action).squeeze(0)\n",
        "\n",
        "    def critic(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the (ensembled) Q-values for the given state-action pair.\n",
        "        \"\"\"\n",
        "        return torch.stack([critic(obs, action) for critic in self.critics], dim=0)\n",
        "\n",
        "    def target_critic(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the (ensembled) target Q-values for the given state-action pair.\n",
        "        \"\"\"\n",
        "        return torch.stack(\n",
        "            [critic(obs, action) for critic in self.target_critics], dim=0\n",
        "        )\n",
        "\n",
        "    def update_critic(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        action: torch.Tensor,\n",
        "        reward: torch.Tensor,\n",
        "        next_obs: torch.Tensor,\n",
        "        done: torch.Tensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update the critic networks by computing target values and minimizing Bellman error.\n",
        "        \"\"\"\n",
        "        (batch_size,) = reward.shape\n",
        "\n",
        "        # Compute target values\n",
        "        # Important: we don't need gradients for target values!\n",
        "        with torch.no_grad():\n",
        "            # TODO\n",
        "            # Sample from the actor\n",
        "            next_action_distribution: torch.distributions.Distribution = self.actor(next_obs)\n",
        "            next_action = next_action_distribution.sample()\n",
        "\n",
        "            # Compute the next Q-values for the sampled actions\n",
        "            next_qs = self.target_critic(next_obs, next_action)\n",
        "\n",
        "            # Handle Q-values from multiple different target critic networks (if necessary)\n",
        "            # (For double-Q, clip-Q, etc.)\n",
        "            next_qs = next_qs.mean(0)[None].expand((self.num_critic_networks, batch_size)).contiguous()\n",
        "\n",
        "            assert next_qs.shape == (\n",
        "                self.num_critic_networks,\n",
        "                batch_size,\n",
        "            ), next_qs.shape\n",
        "\n",
        "            if self.use_entropy_bonus and self.backup_entropy:\n",
        "                # TODO: Add entropy bonus to the target values for SAC\n",
        "                next_action_entropy = self.entropy(next_action_distribution)\n",
        "                next_qs += self.temperature * next_action_entropy\n",
        "\n",
        "            # Compute the target Q-value\n",
        "            target_values = reward + self.discount * (1 - done.type(next_qs.dtype)) * next_qs\n",
        "\n",
        "            assert target_values.shape == (\n",
        "                self.num_critic_networks,\n",
        "                batch_size\n",
        "            )\n",
        "\n",
        "        # TODO: Update the critic\n",
        "        # Predict Q-values\n",
        "        q_values = self.critic(obs, action)\n",
        "        assert q_values.shape == (self.num_critic_networks, batch_size), q_values.shape\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.critic_loss(q_values, target_values)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"critic_loss\": loss.item(),\n",
        "            \"q_values\": q_values.mean().item(),\n",
        "            \"target_values\": target_values.mean().item(),\n",
        "        }\n",
        "\n",
        "    def entropy(self, action_distribution: torch.distributions.Distribution):\n",
        "        \"\"\"\n",
        "        Compute the (approximate) entropy of the action distribution for each batch element.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Compute the entropy of the action distribution.\n",
        "        # Note: Think about whether to use .rsample() or .sample() here...\n",
        "\n",
        "        samples = action_distribution.rsample()\n",
        "        return -action_distribution.log_prob(samples)\n",
        "        # return action_distribution.entropy()\n",
        "\n",
        "    def actor_loss_reinforce(self, obs: torch.Tensor):\n",
        "        batch_size = obs.shape[0]\n",
        "\n",
        "        # TODO: Generate an action distribution\n",
        "        action_distribution: torch.distributions.Distribution = self.actor(obs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO: draw num_actor_samples samples from the action distribution for each batch element\n",
        "            action = action_distribution.sample((self.num_actor_samples,))\n",
        "            assert action.shape == (\n",
        "                self.num_actor_samples,\n",
        "                batch_size,\n",
        "                self.action_dim,\n",
        "            ), action.shape\n",
        "\n",
        "            # TODO: Compute Q-values for the current state-action pair\n",
        "            # q_values = self.critic(obs.repeat((self.num_actor_samples, 1, 1)), action)\n",
        "            q_values = self.critic(obs[None].expand((self.num_actor_samples,\n",
        "                                                      batch_size,\n",
        "                                                      -1)), action)\n",
        "            assert q_values.shape == (\n",
        "                self.num_critic_networks,\n",
        "                self.num_actor_samples,\n",
        "                batch_size,\n",
        "            ), q_values.shape\n",
        "\n",
        "            # Our best guess of the Q-values is the mean of the ensemble\n",
        "            q_values = torch.mean(q_values, axis=0)\n",
        "            advantage = q_values\n",
        "\n",
        "        # Do REINFORCE: calculate log-probs and use the Q-values\n",
        "        # TODO\n",
        "        log_probs = action_distribution.log_prob(action)\n",
        "        loss = -torch.mean(log_probs * advantage)\n",
        "        # loss = 0\n",
        "\n",
        "        return loss, torch.mean(self.entropy(action_distribution))\n",
        "\n",
        "    def actor_loss_reparametrize(self, obs: torch.Tensor):\n",
        "        batch_size = obs.shape[0]\n",
        "\n",
        "        # Sample from the actor\n",
        "        action_distribution: torch.distributions.Distribution = self.actor(obs)\n",
        "\n",
        "        # TODO: Sample actions\n",
        "        # Note: Think about whether to use .rsample() or .sample() here...\n",
        "        action = action_distribution.rsample()\n",
        "\n",
        "        # TODO: Compute Q-values for the sampled state-action pair\n",
        "        q_values = self.critic(obs, action)\n",
        "\n",
        "        # TODO: Compute the actor loss\n",
        "        loss = -torch.mean(q_values)\n",
        "\n",
        "        return loss, torch.mean(self.entropy(action_distribution))\n",
        "\n",
        "    def update_actor(self, obs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Update the actor by one gradient step using either REPARAMETRIZE or REINFORCE.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.actor_gradient_type == \"reparametrize\":\n",
        "            loss, entropy = self.actor_loss_reparametrize(obs)\n",
        "        elif self.actor_gradient_type == \"reinforce\":\n",
        "            loss, entropy = self.actor_loss_reinforce(obs)\n",
        "\n",
        "        # Add entropy if necessary\n",
        "        if self.use_entropy_bonus:\n",
        "            loss -= self.temperature * entropy\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        return {\"actor_loss\": loss.item(), \"entropy\": entropy.item()}\n",
        "\n",
        "    def update_target_critic(self):\n",
        "        self.soft_update_target_critic(1.0)\n",
        "\n",
        "    def soft_update_target_critic(self, tau):\n",
        "        for target_critic, critic in zip(self.target_critics, self.critics):\n",
        "            for target_param, param in zip(\n",
        "                target_critic.parameters(), critic.parameters()\n",
        "            ):\n",
        "                target_param.data.copy_(\n",
        "                    target_param.data * (1.0 - tau) + param.data * tau\n",
        "                )\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        observations: torch.Tensor,\n",
        "        actions: torch.Tensor,\n",
        "        rewards: torch.Tensor,\n",
        "        next_observations: torch.Tensor,\n",
        "        dones: torch.Tensor,\n",
        "        step: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update the actor and critic networks.\n",
        "        \"\"\"\n",
        "\n",
        "        critic_infos = []\n",
        "        # TODO: Update the critic for num_critic_updates steps, and add the output stats to critic_infos\n",
        "        for i in range(self.num_critic_updates):\n",
        "            critic_infos.append(self.update_critic(observations,\n",
        "                                                   actions,\n",
        "                                                   rewards,\n",
        "                                                   next_observations,\n",
        "                                                   dones))\n",
        "\n",
        "        # TODO: Update the actor\n",
        "        actor_info = self.update_actor(observations)\n",
        "\n",
        "        # TODO: Perform either hard or soft target updates.\n",
        "        # Relevant variables:\n",
        "        #  - step\n",
        "        #  - self.target_update_period (None when using soft updates)\n",
        "        #  - self.soft_target_update_rate (None when using hard updates)\n",
        "        if self.target_update_period is None:\n",
        "            self.soft_update_target_critic(self.soft_target_update_rate)\n",
        "        elif step % self.target_update_period == 0:\n",
        "            self.update_target_critic()\n",
        "\n",
        "        # Average the critic info over all of the steps\n",
        "        critic_info = {\n",
        "            k: np.mean([info[k] for info in critic_infos]) for k in critic_infos[0]\n",
        "        }\n",
        "\n",
        "        # Deal with LR scheduling\n",
        "        self.actor_lr_scheduler.step()\n",
        "        self.critic_lr_scheduler.step()\n",
        "\n",
        "        return {\n",
        "            **actor_info,\n",
        "            **critic_info,\n",
        "            \"actor_lr\": self.actor_lr_scheduler.get_last_lr()[0],\n",
        "            \"critic_lr\": self.critic_lr_scheduler.get_last_lr()[0],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vZVR8byyotW"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=1000000):\n",
        "        self.max_size = capacity\n",
        "        self.size = 0\n",
        "        self.observations = None\n",
        "        self.actions = None\n",
        "        self.rewards = None\n",
        "        self.next_observations = None\n",
        "        self.dones = None\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        rand_indices = np.random.randint(0, self.size, size=(batch_size,)) % self.max_size\n",
        "        return {\n",
        "            \"observations\": self.observations[rand_indices],\n",
        "            \"actions\": self.actions[rand_indices],\n",
        "            \"rewards\": self.rewards[rand_indices],\n",
        "            \"next_observations\": self.next_observations[rand_indices],\n",
        "            \"dones\": self.dones[rand_indices],\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def insert(\n",
        "        self,\n",
        "        /,\n",
        "        observation: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "        reward: np.ndarray,\n",
        "        next_observation: np.ndarray,\n",
        "        done: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Insert a single transition into the replay buffer.\n",
        "\n",
        "        Use like:\n",
        "            replay_buffer.insert(\n",
        "                observation=observation,\n",
        "                action=action,\n",
        "                reward=reward,\n",
        "                next_observation=next_observation,\n",
        "                done=done,\n",
        "            )\n",
        "        \"\"\"\n",
        "        if isinstance(reward, (float, int)):\n",
        "            reward = np.array(reward)\n",
        "        if isinstance(done, bool):\n",
        "            done = np.array(done)\n",
        "        if isinstance(action, int):\n",
        "            action = np.array(action, dtype=np.int64)\n",
        "\n",
        "        if self.observations is None:\n",
        "            self.observations = np.empty(\n",
        "                (self.max_size, *observation.shape), dtype=observation.dtype\n",
        "            )\n",
        "            self.actions = np.empty((self.max_size, *action.shape), dtype=action.dtype)\n",
        "            self.rewards = np.empty((self.max_size, *reward.shape), dtype=reward.dtype)\n",
        "            self.next_observations = np.empty(\n",
        "                (self.max_size, *next_observation.shape), dtype=next_observation.dtype\n",
        "            )\n",
        "            self.dones = np.empty((self.max_size, *done.shape), dtype=done.dtype)\n",
        "\n",
        "        assert observation.shape == self.observations.shape[1:]\n",
        "        assert action.shape == self.actions.shape[1:]\n",
        "        assert reward.shape == ()\n",
        "        assert next_observation.shape == self.next_observations.shape[1:]\n",
        "        assert done.shape == ()\n",
        "\n",
        "        self.observations[self.size % self.max_size] = observation\n",
        "        self.actions[self.size % self.max_size] = action\n",
        "        self.rewards[self.size % self.max_size] = reward\n",
        "        self.next_observations[self.size % self.max_size] = next_observation\n",
        "        self.dones[self.size % self.max_size] = done\n",
        "\n",
        "        self.size += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndjJQkDHuhEc"
      },
      "outputs": [],
      "source": [
        "def sample_trajectory(\n",
        "    env: gym.Env, policy, max_length: int, render: bool = False\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Sample a rollout in the environment from a policy.\"\"\"\n",
        "    ob = env.reset()\n",
        "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
        "    steps = 0\n",
        "\n",
        "    while True:\n",
        "        # render an image\n",
        "        if render:\n",
        "            if hasattr(env, \"sim\"):\n",
        "                img = env.sim.render(camera_name=\"track\", height=500, width=500)[::-1]\n",
        "            else:\n",
        "                img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "            if isinstance(img, list):\n",
        "                img = img[0]\n",
        "\n",
        "            image_obs.append(\n",
        "                cv2.resize(img, dsize=(250, 250), interpolation=cv2.INTER_CUBIC)\n",
        "            )\n",
        "\n",
        "        ac = policy.get_action(ob)\n",
        "\n",
        "        next_ob, rew, done, info = env.step(ac)\n",
        "\n",
        "        steps += 1\n",
        "        rollout_done = done or steps > max_length\n",
        "\n",
        "        # record result of taking that action\n",
        "        obs.append(ob)\n",
        "        acs.append(ac)\n",
        "        rewards.append(rew)\n",
        "        next_obs.append(next_ob)\n",
        "        terminals.append(rollout_done)\n",
        "\n",
        "        ob = next_ob  # jump to next timestep\n",
        "\n",
        "        # end the rollout if the rollout ended\n",
        "        if rollout_done:\n",
        "            break\n",
        "\n",
        "    episode_statistics = {\"l\": steps, \"r\": np.sum(rewards)}\n",
        "    if \"episode\" in info:\n",
        "        episode_statistics.update(info[\"episode\"])\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        \"observation\": np.array(obs, dtype=np.float32),\n",
        "        \"image_obs\": np.array(image_obs, dtype=np.uint8),\n",
        "        \"reward\": np.array(rewards, dtype=np.float32),\n",
        "        \"action\": np.array(acs, dtype=np.float32),\n",
        "        \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
        "        \"terminal\": np.array(terminals, dtype=np.float32),\n",
        "        \"episode_statistics\": episode_statistics,\n",
        "    }\n",
        "\n",
        "def sample_n_trajectories(\n",
        "    env: gym.Env, policy, ntraj: int, max_length: int, render: bool = False\n",
        "):\n",
        "    \"\"\"Collect ntraj rollouts.\"\"\"\n",
        "    trajs = []\n",
        "    for _ in range(ntraj):\n",
        "        # collect rollout\n",
        "        traj = sample_trajectory(env, policy, max_length, render)\n",
        "        trajs.append(traj)\n",
        "    return trajs\n",
        "\n",
        "def log_paths_as_videos(paths, max_videos_to_save=2):\n",
        "        # reshape the rollouts\n",
        "        # videos = [np.transpose(p['image_obs'], [0, 3, 1, 2]) for p in paths]\n",
        "        videos = [p['image_obs'] for p in paths]\n",
        "\n",
        "        # max rollout length\n",
        "        max_videos_to_save = np.min([max_videos_to_save, len(videos)])\n",
        "        max_length = videos[0].shape[0]\n",
        "        for i in range(max_videos_to_save):\n",
        "            if videos[i].shape[0]>max_length:\n",
        "                max_length = videos[i].shape[0]\n",
        "\n",
        "        # pad rollouts to all be same length\n",
        "        for i in range(max_videos_to_save):\n",
        "            if videos[i].shape[0]<max_length:\n",
        "                padding = np.tile([videos[i][-1]], (max_length-videos[i].shape[0],1,1,1))\n",
        "                videos[i] = np.concatenate([videos[i], padding], 0)\n",
        "\n",
        "        # log videos to tensorboard event file\n",
        "        videos = np.stack(videos[:max_videos_to_save], 0)\n",
        "\n",
        "        return videos\n",
        "\n",
        "def plot_trajectories(videos):\n",
        "    fig = plt.figure()\n",
        "    imgs = []\n",
        "\n",
        "    n_trajs = videos.shape[0]\n",
        "    for i in range(1, n_trajs + 1):\n",
        "        fig.add_subplot(1, n_trajs, i)\n",
        "        imgs.append(plt.imshow(videos[i - 1, 0, ...]))\n",
        "\n",
        "    plt.close() # this is required to not display the generated image\n",
        "\n",
        "    def init():\n",
        "        for j, im in enumerate(imgs):\n",
        "            im.set_data(videos[j, 0, ...])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def animate(i):\n",
        "        for j, im in enumerate(imgs):\n",
        "            im.set_data(videos[j, i, ...])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    anim = animation.FuncAnimation(fig,\n",
        "                                   animate,\n",
        "                                   init_func=init,\n",
        "                                   frames=videos.shape[1],\n",
        "                                   interval=25,\n",
        "                                   repeat=False)\n",
        "                                #    repeat_delay=1000)\n",
        "\n",
        "    clear_output(True)\n",
        "    display(HTML(anim.to_html5_video()))\n",
        "\n",
        "def run_training_loop(config,\n",
        "                      seed=42):\n",
        "    # set random seeds\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    env = config[\"make_env\"]()\n",
        "    eval_env = config[\"make_env\"]()\n",
        "    render_env = config[\"make_env\"](render=True)\n",
        "\n",
        "    # make the gym environment\n",
        "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "    assert (\n",
        "        not discrete\n",
        "    ), \"Our actor-critic implementation only supports continuous action spaces. (This isn't a fundamental limitation, just a current implementation decision.)\"\n",
        "\n",
        "\n",
        "    ob_shape = env.observation_space.shape\n",
        "    ac_dim = env.action_space.shape[0]\n",
        "\n",
        "    agent = SoftActorCritic(\n",
        "        ob_shape,\n",
        "        ac_dim,\n",
        "        **config[\"agent_kwargs\"],\n",
        "    )\n",
        "\n",
        "    # simulation timestep, will be used for video saving\n",
        "    if \"model\" in dir(env):\n",
        "        fps = 1 / env.model.opt.timestep\n",
        "    elif \"render_fps\" in env.env.metadata:\n",
        "        fps = env.env.metadata[\"render_fps\"]\n",
        "    else:\n",
        "        fps = 4\n",
        "\n",
        "    ep_len = env.spec.max_episode_steps\n",
        "\n",
        "    observation = env.reset()\n",
        "\n",
        "    replay_buffer = ReplayBuffer(config[\"replay_buffer_capacity\"])\n",
        "\n",
        "    stats = {}\n",
        "\n",
        "    t = tqdm.trange(config[\"total_steps\"], dynamic_ncols=True)\n",
        "\n",
        "    for step in t:\n",
        "        if step < config[\"random_steps\"]:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # TODO(student): Select an action\n",
        "            action = agent.get_action(observation)\n",
        "\n",
        "        # Step the environment and add the data to the replay buffer\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        replay_buffer.insert(\n",
        "            observation=observation,\n",
        "            action=action,\n",
        "            reward=reward,\n",
        "            next_observation=next_observation,\n",
        "            done=done and not info.get(\"TimeLimit.truncated\", False),\n",
        "        )\n",
        "\n",
        "        # Handle episode termination\n",
        "        if done:\n",
        "            observation = env.reset()\n",
        "        else:\n",
        "            observation = next_observation\n",
        "\n",
        "        # Main DQN training loop\n",
        "        if step >= config[\"learning_starts\"]:\n",
        "            # TODO: Sample config[\"batch_size\"] samples from the replay buffer\n",
        "            batch = replay_buffer.sample(config[\"batch_size\"])\n",
        "\n",
        "            # TODO: Train the agent. `batch` is a dictionary of numpy arrays,\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            batch = from_numpy(batch)\n",
        "            update_info = agent.update(batch[\"observations\"],\n",
        "                                       batch[\"actions\"],\n",
        "                                       batch[\"rewards\"],\n",
        "                                       batch[\"next_observations\"],\n",
        "                                       batch[\"dones\"],\n",
        "                                       step) #!!!\n",
        "\n",
        "            # Logging code\n",
        "            update_info[\"actor_lr\"] = agent.actor_lr_scheduler.get_last_lr()[0]\n",
        "            update_info[\"critic_lr\"] = agent.critic_lr_scheduler.get_last_lr()[0]\n",
        "\n",
        "            if step % config[\"log_interval\"] == 0:\n",
        "                for k, v in update_info.items():\n",
        "                    stats[k] = v\n",
        "\n",
        "                t.set_postfix(stats, refresh=True)\n",
        "\n",
        "        if step % config[\"eval_interval\"] == 0:\n",
        "            # Evaluate\n",
        "            trajectories = sample_n_trajectories(\n",
        "                eval_env,\n",
        "                agent,\n",
        "                config[\"num_eval_trajectories\"],\n",
        "                ep_len,\n",
        "            )\n",
        "            returns = [t[\"episode_statistics\"][\"r\"] for t in trajectories]\n",
        "            ep_lens = [t[\"episode_statistics\"][\"l\"] for t in trajectories]\n",
        "\n",
        "            stats[\"eval_return\"] = np.mean(returns)\n",
        "            stats[\"eval_ep_len\"] = np.mean(ep_lens)\n",
        "\n",
        "            if len(returns) > 1:\n",
        "                stats[\"eval/return_std\"] = np.std(returns)\n",
        "                stats[\"eval/return_max\"] = np.max(returns)\n",
        "                stats[\"eval/return_min\"] = np.min(returns)\n",
        "                stats[\"eval/ep_len_std\"] = np.std(ep_lens)\n",
        "                stats[\"eval/ep_len_max\"] = np.max(ep_lens)\n",
        "                stats[\"eval/ep_len_min\"] = np.min(ep_lens)\n",
        "\n",
        "                t.set_postfix(stats, refresh=True)\n",
        "\n",
        "    if config[\"num_render_trajectories\"] > 0:\n",
        "        video_trajectories = sample_n_trajectories(\n",
        "            render_env,\n",
        "            agent,\n",
        "            config[\"num_render_trajectories\"],\n",
        "            ep_len,\n",
        "            render=True,\n",
        "        )\n",
        "\n",
        "        videos = log_paths_as_videos(video_trajectories,\n",
        "                                        max_videos_to_save=5)\n",
        "\n",
        "        plot_trajectories(videos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSwdbGHR3G0L"
      },
      "outputs": [],
      "source": [
        "_str_to_activation = {\n",
        "    \"relu\": nn.ReLU(),\n",
        "    \"tanh\": nn.Tanh(),\n",
        "    \"leaky_relu\": nn.LeakyReLU(),\n",
        "    \"sigmoid\": nn.Sigmoid(),\n",
        "    \"selu\": nn.SELU(),\n",
        "    \"softplus\": nn.Softplus(),\n",
        "    \"identity\": nn.Identity(),\n",
        "}\n",
        "\n",
        "def build_mlp(\n",
        "    input_size: int,\n",
        "    output_size: int,\n",
        "    n_layers: int,\n",
        "    size: int,\n",
        "    activation = \"tanh\",\n",
        "    output_activation = \"identity\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds a feedforward neural network\n",
        "\n",
        "    arguments:\n",
        "        input_placeholder: placeholder variable for the state (batch_size, input_size)\n",
        "        scope: variable scope of the network\n",
        "\n",
        "        n_layers: number of hidden layers\n",
        "        size: dimension of each hidden layer\n",
        "        activation: activation of each hidden layer\n",
        "\n",
        "        input_size: size of the input layer\n",
        "        output_size: size of the output layer\n",
        "        output_activation: activation of the output layer\n",
        "\n",
        "    returns:\n",
        "        output_placeholder: the result of a forward pass through the hidden layers + the output layer\n",
        "    \"\"\"\n",
        "    if isinstance(activation, str):\n",
        "        activation = _str_to_activation[activation]\n",
        "    if isinstance(output_activation, str):\n",
        "        output_activation = _str_to_activation[output_activation]\n",
        "    layers = []\n",
        "    in_size = input_size\n",
        "    for _ in range(n_layers):\n",
        "        layers.append(nn.Linear(in_size, size))\n",
        "        layers.append(activation)\n",
        "        in_size = size\n",
        "    layers.append(nn.Linear(in_size, output_size))\n",
        "    layers.append(output_activation)\n",
        "\n",
        "    mlp = nn.Sequential(*layers)\n",
        "    mlp.to(device)\n",
        "    return mlp\n",
        "\n",
        "class MLPPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Base MLP policy, which can take an observation and output a distribution over actions.\n",
        "\n",
        "    This class implements `forward()` which takes a (batched) observation and returns a distribution over actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ac_dim: int,\n",
        "        ob_dim: int,\n",
        "        n_layers: int,\n",
        "        layer_size: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = build_mlp(\n",
        "            input_size=ob_dim,\n",
        "            output_size=2*ac_dim,\n",
        "            n_layers=n_layers,\n",
        "            size=layer_size,\n",
        "        ).to(device)\n",
        "\n",
        "    def make_tanh_transformed(\n",
        "        self, mean: torch.Tensor, std: Union[float, torch.Tensor]\n",
        "    ) -> distributions.Distribution:\n",
        "        if isinstance(std, float):\n",
        "            std = torch.tensor(std, device=mean.device)\n",
        "\n",
        "        if std.shape == ():\n",
        "            std = std.expand(mean.shape)\n",
        "\n",
        "        return distributions.Independent(\n",
        "            distributions.TransformedDistribution(\n",
        "                base_distribution=distributions.Normal(mean, std),\n",
        "                transforms=[distributions.TanhTransform(cache_size=1)],\n",
        "            ),\n",
        "            reinterpreted_batch_ndims=1,\n",
        "        )\n",
        "\n",
        "    def make_multi_normal(\n",
        "        self, mean: torch.Tensor, std: Union[float, torch.Tensor]\n",
        "    ) -> distributions.Distribution:\n",
        "        if isinstance(std, float):\n",
        "            std = torch.tensor(std, device=mean.device)\n",
        "\n",
        "        if std.shape == ():\n",
        "            std = std.expand(mean.shape)\n",
        "\n",
        "        return distributions.Independent(distributions.Normal(mean, std), reinterpreted_batch_ndims=1)\n",
        "\n",
        "    def forward(self, obs: torch.FloatTensor):\n",
        "        \"\"\"\n",
        "        This function defines the forward pass of the network.  You can return anything you want, but you should be\n",
        "        able to differentiate through it. For example, you can return a torch.FloatTensor. You can also return more\n",
        "        flexible objects, such as a `torch.distributions.Distribution` object. It's up to you!\n",
        "        \"\"\"\n",
        "        mean, std = torch.chunk(self.net(obs), 2, dim=-1)\n",
        "        std = torch.nn.functional.softplus(std) + 1e-2\n",
        "\n",
        "        action_distribution = self.make_tanh_transformed(mean, std)\n",
        "\n",
        "        return action_distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdZx6SdJzQ7v"
      },
      "outputs": [],
      "source": [
        "class StateActionCritic(nn.Module):\n",
        "    def __init__(self, ob_dim, ac_dim, n_layers, size):\n",
        "        super().__init__()\n",
        "        self.net = build_mlp(\n",
        "            input_size=ob_dim + ac_dim,\n",
        "            output_size=1,\n",
        "            n_layers=n_layers,\n",
        "            size=size,\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, obs, acs):\n",
        "        return self.net(torch.cat([obs, acs], dim=-1)).squeeze(-1)\n",
        "\n",
        "\n",
        "def make_critic(observation_shape, action_dim) -> nn.Module:\n",
        "    return StateActionCritic(\n",
        "        ob_dim=np.prod(observation_shape),\n",
        "        ac_dim=action_dim,\n",
        "        n_layers=3,\n",
        "        size=128,\n",
        "    )\n",
        "\n",
        "def make_actor(observation_shape, action_dim) -> nn.Module:\n",
        "    assert len(observation_shape) == 1\n",
        "    return MLPPolicy(\n",
        "        ac_dim=action_dim,\n",
        "        ob_dim=np.prod(observation_shape),\n",
        "        n_layers=3,\n",
        "        layer_size=128\n",
        "    )\n",
        "\n",
        "def make_actor_optimizer(params: torch.nn.ParameterList) -> torch.optim.Optimizer:\n",
        "    return torch.optim.Adam(params, lr=3e-4)\n",
        "\n",
        "def make_critic_optimizer(params: torch.nn.ParameterList) -> torch.optim.Optimizer:\n",
        "    return torch.optim.Adam(params, lr=3e-4)\n",
        "\n",
        "def make_lr_schedule(\n",
        "    optimizer: torch.optim.Optimizer,\n",
        ") -> torch.optim.lr_scheduler._LRScheduler:\n",
        "    return torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
        "\n",
        "def make_env(render: bool = False):\n",
        "        return RecordEpisodeStatistics(\n",
        "            ClipAction(\n",
        "                RescaleAction(\n",
        "                    gym.make(\n",
        "                        \"HalfCheetah-v4\", render_mode=\"single_rgb_array\" if render else None #InvertedPendulum-v4\n",
        "                    ),\n",
        "                    -1,\n",
        "                    1,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "agent_kwargs = {\n",
        "            \"make_critic\": make_critic,\n",
        "            \"make_critic_optimizer\": make_critic_optimizer,\n",
        "            \"make_critic_schedule\": make_lr_schedule,\n",
        "            \"make_actor\": make_actor,\n",
        "            \"make_actor_optimizer\": make_actor_optimizer,\n",
        "            \"make_actor_schedule\": make_lr_schedule,\n",
        "            \"num_critic_updates\": 1,\n",
        "            \"discount\": 0.99,\n",
        "            \"actor_gradient_type\": \"reinforce\",\n",
        "            \"num_actor_samples\": 10,\n",
        "            \"num_critic_updates\": 1,\n",
        "            \"num_critic_networks\": 1,\n",
        "            \"target_critic_backup_type\": \"mean\", #mean\n",
        "            \"use_entropy_bonus\": True,\n",
        "            \"backup_entropy\": True,\n",
        "            \"temperature\": 0.2,\n",
        "            \"target_update_period\": None,\n",
        "            \"soft_target_update_rate\": 0.005, #0.005\n",
        "        }\n",
        "\n",
        "total_steps = 1000000\n",
        "\n",
        "config = {\"total_steps\": total_steps,\n",
        "          \"num_render_trajectories\": 3,\n",
        "          \"num_eval_trajectories\": 10,\n",
        "          \"log_interval\": 1000,\n",
        "          \"eval_interval\": 5000,\n",
        "          \"learning_starts\": 10000,\n",
        "          \"batch_size\": 128,\n",
        "          \"replay_buffer_capacity\": 1000000,\n",
        "          \"random_steps\": 5000,\n",
        "          \"make_env\": make_env,\n",
        "          \"agent_kwargs\": agent_kwargs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28OxVrA-4pV",
        "outputId": "891bc0d8-6381-42e3-b17f-bf65d86e5b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            " 71%|███████   | 711071/1000000 [2:38:53<51:56, 92.70it/s, eval_return=980, eval_ep_len=1e+3, eval/return_std=386, eval/return_max=1.26e+3, eval/return_min=-148, eval/ep_len_std=0, eval/ep_len_max=1000, eval/ep_len_min=1000, actor_loss=-38.2, entropy=-.347, critic_loss=6.42, q_values=90.2, target_values=89.9, actor_lr=0.0003, critic_lr=0.0003]"
          ]
        }
      ],
      "source": [
        "run_training_loop(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWrlmUuN_Nq9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}